{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## initializing necessary libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Tensorflow\n",
    "\n",
    "Graphs are constructed with nested mathematical/programmatic operations, usually starting with some sort of constant value as an input.\n",
    "\n",
    "Commands:\n",
    "* tf.constant() - Specifies a constant value\n",
    "* tf.add() - Adds inputted values\n",
    "* tf.subtract() - Subtracts inputted values, in sequence provided\n",
    "* tf.multiply() - Multiplys inputted values\n",
    "* tf.divide() Divides inputted values, as (numerator, denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_value_one = tf.constant(42)\n",
    "input_value_two = tf.constant(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiply_inputs = tf.multiply(input_value_one, input_value_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To 'run' a graph you need to create a session object, and feed the graph to the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_value = session.run(multiply_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n"
     ]
    }
   ],
   "source": [
    "print(output_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data into TenworFlow\n",
    "Placeholders give you a flexible way to bring data into a TensorFlow graph.\n",
    "\n",
    "* tf.placeholder() - Make sure the input is a Numpy array. Also it's *strongly* recommended to set the 'dtype' kwarg to tf.float32\n",
    "\n",
    "Placeholders need a 'feed dictionary' fed to the session to function properly, which is just a dictionary with the key being the placeholder object, and the value being what value(s) the placeholder will have for that run of the session.\n",
    "\n",
    "Placeholders need some kind of shape definition, but you can leave some of the dimensions of an undefined length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    record_date  avg_price  avg_price_prior\n",
      "1    2015-04-02       5.75             5.71\n",
      "2    2015-04-03       5.24             5.75\n",
      "3    2015-04-04       5.74             5.24\n",
      "4    2015-04-05       5.74             5.74\n",
      "5    2015-04-06       5.64             5.74\n",
      "6    2015-04-07       5.66             5.64\n",
      "7    2015-04-08       5.64             5.66\n",
      "8    2015-04-09       5.72             5.64\n",
      "9    2015-04-10       5.48             5.72\n",
      "10   2015-04-11       5.60             5.48\n",
      "11   2015-04-12       5.51             5.60\n",
      "12   2015-04-13       5.50             5.51\n",
      "13   2015-04-14       5.49             5.50\n",
      "14   2015-04-15       5.47             5.49\n",
      "15   2015-04-16       5.46             5.47\n",
      "16   2015-04-17       5.50             5.46\n",
      "17   2015-04-18       5.47             5.50\n",
      "18   2015-04-19       5.60             5.47\n",
      "19   2015-04-20       5.09             5.60\n",
      "20   2015-04-21       5.35             5.09\n",
      "21   2015-04-22       5.00             5.35\n",
      "22   2015-04-23       5.25             5.00\n",
      "23   2015-04-24       5.30             5.25\n",
      "24   2015-04-25       5.01             5.30\n",
      "25   2015-04-26       5.20             5.01\n",
      "26   2015-04-27       5.39             5.20\n",
      "27   2015-04-28       5.05             5.39\n",
      "28   2015-04-29       5.00             5.05\n",
      "29   2015-04-30       5.19             5.00\n",
      "30   2015-05-01       5.05             5.19\n",
      "..          ...        ...              ...\n",
      "885  2017-09-03       5.33             5.32\n",
      "886  2017-09-04       5.36             5.33\n",
      "887  2017-09-05       5.36             5.36\n",
      "888  2017-09-06       5.34             5.36\n",
      "889  2017-09-07       5.36             5.34\n",
      "890  2017-09-08       5.36             5.36\n",
      "891  2017-09-09       5.36             5.36\n",
      "892  2017-09-10       5.40             5.36\n",
      "893  2017-09-11       5.40             5.40\n",
      "894  2017-09-12       5.21             5.40\n",
      "895  2017-09-13       5.49             5.21\n",
      "896  2017-09-14       5.58             5.49\n",
      "897  2017-09-15       5.75             5.58\n",
      "898  2017-09-16       5.75             5.75\n",
      "899  2017-09-17       5.65             5.75\n",
      "900  2017-09-18       5.69             5.65\n",
      "901  2017-09-19       5.68             5.69\n",
      "902  2017-09-20       5.69             5.68\n",
      "903  2017-09-21       5.51             5.69\n",
      "904  2017-09-22       5.43             5.51\n",
      "905  2017-09-23       5.40             5.43\n",
      "906  2017-09-24       5.40             5.40\n",
      "907  2017-09-25       5.20             5.40\n",
      "908  2017-09-26       5.30             5.20\n",
      "909  2017-09-27       5.35             5.30\n",
      "910  2017-09-28       5.79             5.35\n",
      "911  2017-09-29       5.20             5.79\n",
      "912  2017-09-30       5.20             5.20\n",
      "913  2017-10-01       5.24             5.20\n",
      "914  2017-10-02       5.30             5.24\n",
      "\n",
      "[914 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "## Importing working data\n",
    "dataset = pd.read_csv('test_data//minerals_verge_market.csv')\n",
    "working_data = dataset.loc[dataset['type_name'] == 'Tritanium', ['record_date', 'avg_price']]\n",
    "working_data = working_data.join(\n",
    "    working_data['avg_price'].shift(1),\n",
    "    rsuffix = '_prior'\n",
    ")\n",
    "working_data = working_data.loc[np.all(pd.notnull(working_data), axis = 1), :]\n",
    "print(working_data)\n",
    "feature_cols = ['avg_price_prior']\n",
    "label_cols = ['avg_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ph = tf.placeholder(shape = (None, len(feature_cols)), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed = {\n",
    "    input_ph: working_data[feature_cols].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.85500002]\n",
      " [ 2.875     ]\n",
      " [ 2.61999989]\n",
      " [ 2.86999989]\n",
      " [ 2.86999989]\n",
      " [ 2.81999993]\n",
      " [ 2.82999992]\n",
      " [ 2.81999993]\n",
      " [ 2.8599999 ]\n",
      " [ 2.74000001]\n",
      " [ 2.79999995]\n",
      " [ 2.75500011]\n",
      " [ 2.75      ]\n",
      " [ 2.74499989]\n",
      " [ 2.7349999 ]\n",
      " [ 2.73000002]\n",
      " [ 2.75      ]\n",
      " [ 2.7349999 ]\n",
      " [ 2.79999995]\n",
      " [ 2.54500008]\n",
      " [ 2.67499995]\n",
      " [ 2.5       ]\n",
      " [ 2.625     ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.50500011]\n",
      " [ 2.5999999 ]\n",
      " [ 2.69499993]\n",
      " [ 2.5250001 ]\n",
      " [ 2.5       ]\n",
      " [ 2.59500003]\n",
      " [ 2.5250001 ]\n",
      " [ 2.49499989]\n",
      " [ 2.50500011]\n",
      " [ 2.49499989]\n",
      " [ 2.49000001]\n",
      " [ 2.17499995]\n",
      " [ 2.19000006]\n",
      " [ 2.19000006]\n",
      " [ 2.45000005]\n",
      " [ 2.45000005]\n",
      " [ 2.5       ]\n",
      " [ 2.5       ]\n",
      " [ 2.54999995]\n",
      " [ 2.33999991]\n",
      " [ 2.4000001 ]\n",
      " [ 2.48000002]\n",
      " [ 2.4000001 ]\n",
      " [ 2.41000009]\n",
      " [ 2.38000011]\n",
      " [ 2.4849999 ]\n",
      " [ 2.38499999]\n",
      " [ 2.4749999 ]\n",
      " [ 2.49000001]\n",
      " [ 2.47000003]\n",
      " [ 2.42499995]\n",
      " [ 2.3900001 ]\n",
      " [ 2.4749999 ]\n",
      " [ 2.49499989]\n",
      " [ 2.54999995]\n",
      " [ 2.3499999 ]\n",
      " [ 2.5       ]\n",
      " [ 2.42499995]\n",
      " [ 2.49499989]\n",
      " [ 2.05999994]\n",
      " [ 2.4849999 ]\n",
      " [ 2.4849999 ]\n",
      " [ 2.36500001]\n",
      " [ 2.45000005]\n",
      " [ 2.4849999 ]\n",
      " [ 2.5999999 ]\n",
      " [ 2.5       ]\n",
      " [ 2.4749999 ]\n",
      " [ 2.47000003]\n",
      " [ 2.47000003]\n",
      " [ 2.47000003]\n",
      " [ 2.49499989]\n",
      " [ 2.43499994]\n",
      " [ 2.43499994]\n",
      " [ 2.46499991]\n",
      " [ 2.43000007]\n",
      " [ 2.36500001]\n",
      " [ 2.33500004]\n",
      " [ 2.42000008]\n",
      " [ 2.4000001 ]\n",
      " [ 2.34500003]\n",
      " [ 2.4749999 ]\n",
      " [ 2.41000009]\n",
      " [ 2.42000008]\n",
      " [ 2.45000005]\n",
      " [ 2.46000004]\n",
      " [ 2.46499991]\n",
      " [ 2.4749999 ]\n",
      " [ 2.49499989]\n",
      " [ 2.3499999 ]\n",
      " [ 2.4749999 ]\n",
      " [ 2.44499993]\n",
      " [ 2.40499997]\n",
      " [ 2.36999989]\n",
      " [ 2.4749999 ]\n",
      " [ 2.5999999 ]\n",
      " [ 2.54999995]\n",
      " [ 2.49499989]\n",
      " [ 2.40499997]\n",
      " [ 2.38499999]\n",
      " [ 2.41000009]\n",
      " [ 2.49499989]\n",
      " [ 2.41499996]\n",
      " [ 2.43499994]\n",
      " [ 2.41499996]\n",
      " [ 2.49499989]\n",
      " [ 2.49499989]\n",
      " [ 2.41499996]\n",
      " [ 2.41499996]\n",
      " [ 2.54500008]\n",
      " [ 2.00500011]\n",
      " [ 2.42499995]\n",
      " [ 2.42499995]\n",
      " [ 2.5       ]\n",
      " [ 2.44499993]\n",
      " [ 2.46000004]\n",
      " [ 2.46000004]\n",
      " [ 2.45499992]\n",
      " [ 2.45499992]\n",
      " [ 2.46000004]\n",
      " [ 2.29999995]\n",
      " [ 2.5       ]\n",
      " [ 2.45000005]\n",
      " [ 2.5       ]\n",
      " [ 2.29999995]\n",
      " [ 2.42499995]\n",
      " [ 2.2650001 ]\n",
      " [ 2.29999995]\n",
      " [ 2.30500007]\n",
      " [ 2.49499989]\n",
      " [ 2.5       ]\n",
      " [ 2.49499989]\n",
      " [ 2.49499989]\n",
      " [ 2.4749999 ]\n",
      " [ 2.3499999 ]\n",
      " [ 2.4000001 ]\n",
      " [ 2.28500009]\n",
      " [ 2.28500009]\n",
      " [ 2.42499995]\n",
      " [ 2.3599999 ]\n",
      " [ 2.36500001]\n",
      " [ 2.29500008]\n",
      " [ 2.5       ]\n",
      " [ 2.5       ]\n",
      " [ 2.4749999 ]\n",
      " [ 2.36999989]\n",
      " [ 2.49000001]\n",
      " [ 2.5       ]\n",
      " [ 2.49000001]\n",
      " [ 2.5       ]\n",
      " [ 2.54999995]\n",
      " [ 2.5       ]\n",
      " [ 2.41499996]\n",
      " [ 2.5       ]\n",
      " [ 2.41499996]\n",
      " [ 2.49499989]\n",
      " [ 2.43499994]\n",
      " [ 2.46000004]\n",
      " [ 2.41499996]\n",
      " [ 2.44499993]\n",
      " [ 2.02999997]\n",
      " [ 2.46000004]\n",
      " [ 2.3499999 ]\n",
      " [ 2.5250001 ]\n",
      " [ 2.45000005]\n",
      " [ 2.30500007]\n",
      " [ 2.41000009]\n",
      " [ 2.5       ]\n",
      " [ 2.5       ]\n",
      " [ 2.42000008]\n",
      " [ 2.55500007]\n",
      " [ 2.5       ]\n",
      " [ 2.5999999 ]\n",
      " [ 2.5150001 ]\n",
      " [ 2.51999998]\n",
      " [ 2.71000004]\n",
      " [ 2.54999995]\n",
      " [ 2.44000006]\n",
      " [ 2.4749999 ]\n",
      " [ 2.71000004]\n",
      " [ 2.44499993]\n",
      " [ 2.71000004]\n",
      " [ 2.56500006]\n",
      " [ 2.74000001]\n",
      " [ 2.67499995]\n",
      " [ 2.44000006]\n",
      " [ 2.6500001 ]\n",
      " [ 2.99499989]\n",
      " [ 2.65499997]\n",
      " [ 2.7249999 ]\n",
      " [ 2.7750001 ]\n",
      " [ 2.66499996]\n",
      " [ 2.66499996]\n",
      " [ 2.84500003]\n",
      " [ 2.86500001]\n",
      " [ 2.70000005]\n",
      " [ 3.        ]\n",
      " [ 2.72000003]\n",
      " [ 2.89499998]\n",
      " [ 2.7249999 ]\n",
      " [ 2.95000005]\n",
      " [ 2.7349999 ]\n",
      " [ 2.75      ]\n",
      " [ 2.81999993]\n",
      " [ 2.79500008]\n",
      " [ 2.9000001 ]\n",
      " [ 2.81500006]\n",
      " [ 2.95000005]\n",
      " [ 2.82999992]\n",
      " [ 2.91000009]\n",
      " [ 2.91499996]\n",
      " [ 2.91499996]\n",
      " [ 2.875     ]\n",
      " [ 2.90499997]\n",
      " [ 2.91499996]\n",
      " [ 2.82999992]\n",
      " [ 2.7750001 ]\n",
      " [ 1.59500003]\n",
      " [ 2.82999992]\n",
      " [ 2.8499999 ]\n",
      " [ 2.8900001 ]\n",
      " [ 3.        ]\n",
      " [ 3.18000007]\n",
      " [ 3.        ]\n",
      " [ 2.99499989]\n",
      " [ 2.99000001]\n",
      " [ 2.8900001 ]\n",
      " [ 2.90499997]\n",
      " [ 2.91499996]\n",
      " [ 2.92000008]\n",
      " [ 2.92499995]\n",
      " [ 2.91000009]\n",
      " [ 2.92499995]\n",
      " [ 2.90499997]\n",
      " [ 2.88000011]\n",
      " [ 2.9000001 ]\n",
      " [ 2.80999994]\n",
      " [ 2.81500006]\n",
      " [ 2.89499998]\n",
      " [ 2.88499999]\n",
      " [ 2.92499995]\n",
      " [ 3.04500008]\n",
      " [ 2.99499989]\n",
      " [ 2.91000009]\n",
      " [ 2.93499994]\n",
      " [ 3.        ]\n",
      " [ 3.02999997]\n",
      " [ 2.95000005]\n",
      " [ 3.        ]\n",
      " [ 2.93000007]\n",
      " [ 2.93000007]\n",
      " [ 2.9000001 ]\n",
      " [ 2.90499997]\n",
      " [ 2.90499997]\n",
      " [ 2.90499997]\n",
      " [ 3.        ]\n",
      " [ 2.91000009]\n",
      " [ 2.92499995]\n",
      " [ 2.93499994]\n",
      " [ 2.93499994]\n",
      " [ 2.92000008]\n",
      " [ 2.94000006]\n",
      " [ 2.94000006]\n",
      " [ 2.94000006]\n",
      " [ 2.94000006]\n",
      " [ 2.95000005]\n",
      " [ 2.875     ]\n",
      " [ 2.9000001 ]\n",
      " [ 2.8900001 ]\n",
      " [ 2.7249999 ]\n",
      " [ 2.90499997]\n",
      " [ 2.95000005]\n",
      " [ 2.91000009]\n",
      " [ 2.9000001 ]\n",
      " [ 2.875     ]\n",
      " [ 2.875     ]\n",
      " [ 2.9749999 ]\n",
      " [ 2.91000009]\n",
      " [ 2.95000005]\n",
      " [ 2.91000009]\n",
      " [ 2.92499995]\n",
      " [ 2.92499995]\n",
      " [ 2.94000006]\n",
      " [ 3.        ]\n",
      " [ 2.95499992]\n",
      " [ 2.96000004]\n",
      " [ 3.0999999 ]\n",
      " [ 2.92000008]\n",
      " [ 3.        ]\n",
      " [ 2.80500007]\n",
      " [ 2.80500007]\n",
      " [ 2.80500007]\n",
      " [ 2.92000008]\n",
      " [ 3.08500004]\n",
      " [ 2.92000008]\n",
      " [ 2.7750001 ]\n",
      " [ 3.0999999 ]\n",
      " [ 3.04999995]\n",
      " [ 2.93499994]\n",
      " [ 2.96000004]\n",
      " [ 2.96000004]\n",
      " [ 2.95499992]\n",
      " [ 3.        ]\n",
      " [ 2.91499996]\n",
      " [ 2.92499995]\n",
      " [ 2.98000002]\n",
      " [ 3.        ]\n",
      " [ 2.93499994]\n",
      " [ 2.95499992]\n",
      " [ 2.93000007]\n",
      " [ 2.85500002]\n",
      " [ 2.93499994]\n",
      " [ 2.95000005]\n",
      " [ 2.96000004]\n",
      " [ 2.93499994]\n",
      " [ 2.82500005]\n",
      " [ 2.7750001 ]\n",
      " [ 2.93000007]\n",
      " [ 2.79500008]\n",
      " [ 2.80500007]\n",
      " [ 2.85500002]\n",
      " [ 2.88000011]\n",
      " [ 2.8900001 ]\n",
      " [ 2.89499998]\n",
      " [ 2.91499996]\n",
      " [ 2.9749999 ]\n",
      " [ 2.9749999 ]\n",
      " [ 2.92499995]\n",
      " [ 2.88000011]\n",
      " [ 2.88499999]\n",
      " [ 2.88499999]\n",
      " [ 2.88499999]\n",
      " [ 2.99000001]\n",
      " [ 2.7349999 ]\n",
      " [ 2.95000005]\n",
      " [ 2.79999995]\n",
      " [ 2.82500005]\n",
      " [ 2.68499994]\n",
      " [ 2.84500003]\n",
      " [ 2.85500002]\n",
      " [ 2.85500002]\n",
      " [ 2.8499999 ]\n",
      " [ 2.85500002]\n",
      " [ 2.8599999 ]\n",
      " [ 2.82999992]\n",
      " [ 2.82500005]\n",
      " [ 2.82500005]\n",
      " [ 2.84500003]\n",
      " [ 2.88499999]\n",
      " [ 2.85500002]\n",
      " [ 2.83999991]\n",
      " [ 2.94499993]\n",
      " [ 2.85500002]\n",
      " [ 2.86500001]\n",
      " [ 2.99499989]\n",
      " [ 2.88000011]\n",
      " [ 2.88000011]\n",
      " [ 2.9000001 ]\n",
      " [ 2.90499997]\n",
      " [ 2.90499997]\n",
      " [ 2.9000001 ]\n",
      " [ 2.8900001 ]\n",
      " [ 2.99499989]\n",
      " [ 2.91000009]\n",
      " [ 3.        ]\n",
      " [ 2.98000002]\n",
      " [ 2.95499992]\n",
      " [ 2.96000004]\n",
      " [ 3.        ]\n",
      " [ 2.93000007]\n",
      " [ 2.93000007]\n",
      " [ 3.05999994]\n",
      " [ 2.94000006]\n",
      " [ 3.        ]\n",
      " [ 2.94499993]\n",
      " [ 2.94000006]\n",
      " [ 2.94000006]\n",
      " [ 2.94000006]\n",
      " [ 2.9000001 ]\n",
      " [ 2.95000005]\n",
      " [ 2.92000008]\n",
      " [ 2.92000008]\n",
      " [ 2.92000008]\n",
      " [ 2.92000008]\n",
      " [ 2.92000008]\n",
      " [ 2.90499997]\n",
      " [ 2.99000001]\n",
      " [ 2.8499999 ]\n",
      " [ 2.91499996]\n",
      " [ 3.04999995]\n",
      " [ 2.91499996]\n",
      " [ 3.04999995]\n",
      " [ 2.91499996]\n",
      " [ 2.91499996]\n",
      " [ 2.91499996]\n",
      " [ 2.91499996]\n",
      " [ 2.9749999 ]\n",
      " [ 2.92000008]\n",
      " [ 2.91499996]\n",
      " [ 2.91499996]\n",
      " [ 2.92000008]\n",
      " [ 2.92000008]\n",
      " [ 3.09500003]\n",
      " [ 3.        ]\n",
      " [ 2.90499997]\n",
      " [ 2.90499997]\n",
      " [ 2.875     ]\n",
      " [ 2.9000001 ]\n",
      " [ 2.78500009]\n",
      " [ 2.79999995]\n",
      " [ 2.5       ]\n",
      " [ 2.45000005]\n",
      " [ 2.71000004]\n",
      " [ 2.69000006]\n",
      " [ 2.70499992]\n",
      " [ 2.78999996]\n",
      " [ 2.79500008]\n",
      " [ 2.68000007]\n",
      " [ 2.78999996]\n",
      " [ 2.81500006]\n",
      " [ 2.75      ]\n",
      " [ 2.78999996]\n",
      " [ 2.79999995]\n",
      " [ 2.92499995]\n",
      " [ 2.67499995]\n",
      " [ 2.70499992]\n",
      " [ 2.70499992]\n",
      " [ 2.7750001 ]\n",
      " [ 2.75      ]\n",
      " [ 2.54999995]\n",
      " [ 2.54999995]\n",
      " [ 2.54999995]\n",
      " [ 2.54999995]\n",
      " [ 2.66499996]\n",
      " [ 2.625     ]\n",
      " [ 2.55500007]\n",
      " [ 2.75      ]\n",
      " [ 2.54999995]\n",
      " [ 2.50500011]\n",
      " [ 2.50500011]\n",
      " [ 2.4749999 ]\n",
      " [ 2.55999994]\n",
      " [ 2.5999999 ]\n",
      " [ 2.625     ]\n",
      " [ 2.4749999 ]\n",
      " [ 2.74000001]\n",
      " [ 2.17499995]\n",
      " [ 2.5999999 ]\n",
      " [ 2.5       ]\n",
      " [ 2.94000006]\n",
      " [ 2.50500011]\n",
      " [ 2.50500011]\n",
      " [ 2.50500011]\n",
      " [ 2.6400001 ]\n",
      " [ 2.51999998]\n",
      " [ 2.51999998]\n",
      " [ 2.5150001 ]\n",
      " [ 2.5150001 ]\n",
      " [ 2.5150001 ]\n",
      " [ 2.5150001 ]\n",
      " [ 2.5150001 ]\n",
      " [ 2.7750001 ]\n",
      " [ 2.53500009]\n",
      " [ 2.53500009]\n",
      " [ 2.53500009]\n",
      " [ 2.82500005]\n",
      " [ 2.52999997]\n",
      " [ 2.53500009]\n",
      " [ 2.53500009]\n",
      " [ 2.53999996]\n",
      " [ 2.53999996]\n",
      " [ 2.53999996]\n",
      " [ 2.5999999 ]\n",
      " [ 2.5999999 ]\n",
      " [ 2.5       ]\n",
      " [ 2.53999996]\n",
      " [ 2.5999999 ]\n",
      " [ 2.53999996]\n",
      " [ 3.11999989]\n",
      " [ 2.53999996]\n",
      " [ 2.27999997]\n",
      " [ 3.        ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.53999996]\n",
      " [ 3.        ]\n",
      " [ 2.54500008]\n",
      " [ 2.53999996]\n",
      " [ 2.53999996]\n",
      " [ 2.67499995]\n",
      " [ 2.54999995]\n",
      " [ 2.68499994]\n",
      " [ 2.67499995]\n",
      " [ 2.5       ]\n",
      " [ 2.50500011]\n",
      " [ 2.5999999 ]\n",
      " [ 2.50500011]\n",
      " [ 2.67499995]\n",
      " [ 2.6500001 ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.5250001 ]\n",
      " [ 2.5250001 ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.5       ]\n",
      " [ 2.57500005]\n",
      " [ 2.54999995]\n",
      " [ 2.5       ]\n",
      " [ 2.60500002]\n",
      " [ 2.63000011]\n",
      " [ 2.625     ]\n",
      " [ 2.6400001 ]\n",
      " [ 2.8499999 ]\n",
      " [ 2.55500007]\n",
      " [ 2.70000005]\n",
      " [ 2.55500007]\n",
      " [ 2.61999989]\n",
      " [ 2.61999989]\n",
      " [ 2.625     ]\n",
      " [ 2.5999999 ]\n",
      " [ 2.6400001 ]\n",
      " [ 2.65499997]\n",
      " [ 2.65499997]\n",
      " [ 2.68000007]\n",
      " [ 2.70000005]\n",
      " [ 2.66000009]\n",
      " [ 2.71499991]\n",
      " [ 2.7349999 ]\n",
      " [ 2.75      ]\n",
      " [ 2.7349999 ]\n",
      " [ 2.76999998]\n",
      " [ 2.60500002]\n",
      " [ 2.63000011]\n",
      " [ 2.66499996]\n",
      " [ 2.67000008]\n",
      " [ 2.67000008]\n",
      " [ 2.65499997]\n",
      " [ 2.6400001 ]\n",
      " [ 2.9000001 ]\n",
      " [ 2.71499991]\n",
      " [ 2.6400001 ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.65499997]\n",
      " [ 2.72000003]\n",
      " [ 2.66499996]\n",
      " [ 2.71499991]\n",
      " [ 2.72000003]\n",
      " [ 2.85500002]\n",
      " [ 2.71000004]\n",
      " [ 2.63000011]\n",
      " [ 2.72000003]\n",
      " [ 2.69000006]\n",
      " [ 2.66499996]\n",
      " [ 2.66499996]\n",
      " [ 2.66499996]\n",
      " [ 2.82500005]\n",
      " [ 2.67499995]\n",
      " [ 2.76999998]\n",
      " [ 2.66499996]\n",
      " [ 2.76999998]\n",
      " [ 2.76999998]\n",
      " [ 2.7249999 ]\n",
      " [ 2.67000008]\n",
      " [ 2.67000008]\n",
      " [ 2.66499996]\n",
      " [ 2.72000003]\n",
      " [ 2.67499995]\n",
      " [ 2.67499995]\n",
      " [ 2.67499995]\n",
      " [ 2.71499991]\n",
      " [ 2.68000007]\n",
      " [ 2.67499995]\n",
      " [ 3.        ]\n",
      " [ 2.7750001 ]\n",
      " [ 2.67499995]\n",
      " [ 2.76999998]\n",
      " [ 2.79500008]\n",
      " [ 2.68499994]\n",
      " [ 2.76999998]\n",
      " [ 2.68499994]\n",
      " [ 2.68499994]\n",
      " [ 2.68499994]\n",
      " [ 2.72000003]\n",
      " [ 2.72000003]\n",
      " [ 2.69000006]\n",
      " [ 2.68499994]\n",
      " [ 2.69000006]\n",
      " [ 2.68499994]\n",
      " [ 2.95000005]\n",
      " [ 2.68499994]\n",
      " [ 2.5999999 ]\n",
      " [ 2.74000001]\n",
      " [ 2.71499991]\n",
      " [ 2.5       ]\n",
      " [ 2.75500011]\n",
      " [ 2.75500011]\n",
      " [ 2.77999997]\n",
      " [ 2.79999995]\n",
      " [ 2.75999999]\n",
      " [ 2.5250001 ]\n",
      " [ 2.7750001 ]\n",
      " [ 2.75      ]\n",
      " [ 2.70000005]\n",
      " [ 2.53999996]\n",
      " [ 2.57500005]\n",
      " [ 2.54999995]\n",
      " [ 2.54999995]\n",
      " [ 2.5999999 ]\n",
      " [ 2.5999999 ]\n",
      " [ 2.57500005]\n",
      " [ 2.5999999 ]\n",
      " [ 2.57500005]\n",
      " [ 2.5999999 ]\n",
      " [ 2.58500004]\n",
      " [ 2.45000005]\n",
      " [ 2.32999992]\n",
      " [ 2.5       ]\n",
      " [ 2.79999995]\n",
      " [ 2.5       ]\n",
      " [ 2.5       ]\n",
      " [ 2.50500011]\n",
      " [ 2.52999997]\n",
      " [ 2.54999995]\n",
      " [ 2.5       ]\n",
      " [ 2.4849999 ]\n",
      " [ 2.4849999 ]\n",
      " [ 2.04999995]\n",
      " [ 2.38499999]\n",
      " [ 2.35500002]\n",
      " [ 2.4000001 ]\n",
      " [ 2.25999999]\n",
      " [ 2.3900001 ]\n",
      " [ 2.39499998]\n",
      " [ 2.35500002]\n",
      " [ 2.27999997]\n",
      " [ 2.57500005]\n",
      " [ 2.1500001 ]\n",
      " [ 2.1500001 ]\n",
      " [ 2.17000008]\n",
      " [ 2.1500001 ]\n",
      " [ 2.25      ]\n",
      " [ 2.3499999 ]\n",
      " [ 2.24499989]\n",
      " [ 2.07500005]\n",
      " [ 2.20000005]\n",
      " [ 2.125     ]\n",
      " [ 2.08500004]\n",
      " [ 2.08500004]\n",
      " [ 2.0999999 ]\n",
      " [ 2.24499989]\n",
      " [ 2.17000008]\n",
      " [ 2.20499992]\n",
      " [ 2.2750001 ]\n",
      " [ 2.        ]\n",
      " [ 2.27999997]\n",
      " [ 2.1500001 ]\n",
      " [ 2.25      ]\n",
      " [ 2.27999997]\n",
      " [ 2.2249999 ]\n",
      " [ 2.07999992]\n",
      " [ 2.07999992]\n",
      " [ 2.08500004]\n",
      " [ 2.00500011]\n",
      " [ 2.2249999 ]\n",
      " [ 2.15499997]\n",
      " [ 2.02999997]\n",
      " [ 2.01999998]\n",
      " [ 2.61999989]\n",
      " [ 2.07500005]\n",
      " [ 2.07999992]\n",
      " [ 2.1500001 ]\n",
      " [ 2.00999999]\n",
      " [ 2.03500009]\n",
      " [ 2.09500003]\n",
      " [ 2.0999999 ]\n",
      " [ 2.05500007]\n",
      " [ 2.0999999 ]\n",
      " [ 2.0999999 ]\n",
      " [ 2.0999999 ]\n",
      " [ 2.09500003]\n",
      " [ 2.09500003]\n",
      " [ 2.0999999 ]\n",
      " [ 2.0999999 ]\n",
      " [ 2.0999999 ]\n",
      " [ 2.09500003]\n",
      " [ 2.03999996]\n",
      " [ 2.03999996]\n",
      " [ 2.        ]\n",
      " [ 1.995     ]\n",
      " [ 1.93499994]\n",
      " [ 1.96000004]\n",
      " [ 1.92999995]\n",
      " [ 1.95500004]\n",
      " [ 2.0150001 ]\n",
      " [ 1.96500003]\n",
      " [ 2.04999995]\n",
      " [ 1.86500001]\n",
      " [ 1.96500003]\n",
      " [ 1.96500003]\n",
      " [ 2.        ]\n",
      " [ 2.00500011]\n",
      " [ 2.0999999 ]\n",
      " [ 2.0150001 ]\n",
      " [ 2.03500009]\n",
      " [ 2.00500011]\n",
      " [ 1.96500003]\n",
      " [ 2.04999995]\n",
      " [ 1.97500002]\n",
      " [ 1.98000002]\n",
      " [ 2.        ]\n",
      " [ 1.97500002]\n",
      " [ 1.97500002]\n",
      " [ 2.0150001 ]\n",
      " [ 1.995     ]\n",
      " [ 1.98000002]\n",
      " [ 1.97500002]\n",
      " [ 2.0150001 ]\n",
      " [ 1.995     ]\n",
      " [ 1.995     ]\n",
      " [ 1.995     ]\n",
      " [ 1.995     ]\n",
      " [ 2.00500011]\n",
      " [ 2.00500011]\n",
      " [ 2.00500011]\n",
      " [ 2.00500011]\n",
      " [ 2.0150001 ]\n",
      " [ 2.2750001 ]\n",
      " [ 1.85000002]\n",
      " [ 2.04999995]\n",
      " [ 2.00999999]\n",
      " [ 1.95000005]\n",
      " [ 2.        ]\n",
      " [ 2.00500011]\n",
      " [ 1.95000005]\n",
      " [ 2.00999999]\n",
      " [ 2.00500011]\n",
      " [ 2.00500011]\n",
      " [ 2.00999999]\n",
      " [ 2.00500011]\n",
      " [ 2.00999999]\n",
      " [ 2.0150001 ]\n",
      " [ 2.02999997]\n",
      " [ 2.03500009]\n",
      " [ 2.02999997]\n",
      " [ 2.02999997]\n",
      " [ 2.02999997]\n",
      " [ 2.0150001 ]\n",
      " [ 2.11500001]\n",
      " [ 2.0150001 ]\n",
      " [ 2.06999993]\n",
      " [ 2.16499996]\n",
      " [ 2.03999996]\n",
      " [ 2.11500001]\n",
      " [ 2.1400001 ]\n",
      " [ 2.13499999]\n",
      " [ 2.05500007]\n",
      " [ 2.1500001 ]\n",
      " [ 2.03999996]\n",
      " [ 2.17499995]\n",
      " [ 2.06500006]\n",
      " [ 2.25      ]\n",
      " [ 2.08500004]\n",
      " [ 2.125     ]\n",
      " [ 2.03999996]\n",
      " [ 2.10500002]\n",
      " [ 2.15499997]\n",
      " [ 2.16000009]\n",
      " [ 2.07999992]\n",
      " [ 2.06500006]\n",
      " [ 2.05999994]\n",
      " [ 2.06999993]\n",
      " [ 2.07999992]\n",
      " [ 2.03500009]\n",
      " [ 2.07999992]\n",
      " [ 2.08500004]\n",
      " [ 2.0999999 ]\n",
      " [ 2.0999999 ]\n",
      " [ 2.10500002]\n",
      " [ 2.1099999 ]\n",
      " [ 2.0999999 ]\n",
      " [ 2.0999999 ]\n",
      " [ 2.0999999 ]\n",
      " [ 2.13000011]\n",
      " [ 2.10500002]\n",
      " [ 2.10500002]\n",
      " [ 2.0999999 ]\n",
      " [ 2.1099999 ]\n",
      " [ 2.11999989]\n",
      " [ 2.1099999 ]\n",
      " [ 2.14499998]\n",
      " [ 2.20000005]\n",
      " [ 2.1099999 ]\n",
      " [ 2.11999989]\n",
      " [ 2.1500001 ]\n",
      " [ 2.13000011]\n",
      " [ 2.13499999]\n",
      " [ 2.17000008]\n",
      " [ 2.18499994]\n",
      " [ 2.2249999 ]\n",
      " [ 2.19000006]\n",
      " [ 2.20000005]\n",
      " [ 2.21499991]\n",
      " [ 2.24000001]\n",
      " [ 2.29500008]\n",
      " [ 2.35500002]\n",
      " [ 2.49499989]\n",
      " [ 2.34500003]\n",
      " [ 2.4000001 ]\n",
      " [ 2.3599999 ]\n",
      " [ 2.36500001]\n",
      " [ 2.5       ]\n",
      " [ 2.5       ]\n",
      " [ 2.375     ]\n",
      " [ 2.5       ]\n",
      " [ 2.38499999]\n",
      " [ 2.625     ]\n",
      " [ 2.4000001 ]\n",
      " [ 2.35500002]\n",
      " [ 2.4000001 ]\n",
      " [ 2.40499997]\n",
      " [ 2.54500008]\n",
      " [ 2.54999995]\n",
      " [ 2.44000006]\n",
      " [ 2.5       ]\n",
      " [ 2.61999989]\n",
      " [ 2.44499993]\n",
      " [ 2.5       ]\n",
      " [ 2.5       ]\n",
      " [ 2.5       ]\n",
      " [ 2.5       ]\n",
      " [ 2.46499991]\n",
      " [ 2.5       ]\n",
      " [ 2.46000004]\n",
      " [ 2.47000003]\n",
      " [ 2.5       ]\n",
      " [ 2.48000002]\n",
      " [ 2.47000003]\n",
      " [ 2.47000003]\n",
      " [ 2.47000003]\n",
      " [ 2.46000004]\n",
      " [ 2.625     ]\n",
      " [ 2.4849999 ]\n",
      " [ 2.5150001 ]\n",
      " [ 2.5250001 ]\n",
      " [ 2.5150001 ]\n",
      " [ 2.51999998]\n",
      " [ 2.5150001 ]\n",
      " [ 2.5150001 ]\n",
      " [ 2.5150001 ]\n",
      " [ 2.5150001 ]\n",
      " [ 1.98500001]\n",
      " [ 2.52999997]\n",
      " [ 2.75      ]\n",
      " [ 2.54999995]\n",
      " [ 2.52999997]\n",
      " [ 2.52999997]\n",
      " [ 2.75      ]\n",
      " [ 2.55500007]\n",
      " [ 2.55500007]\n",
      " [ 2.54500008]\n",
      " [ 2.57500005]\n",
      " [ 2.57500005]\n",
      " [ 2.54500008]\n",
      " [ 2.54500008]\n",
      " [ 2.75      ]\n",
      " [ 2.74000001]\n",
      " [ 2.6500001 ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.75500011]\n",
      " [ 2.6500001 ]\n",
      " [ 2.74499989]\n",
      " [ 2.5999999 ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.65499997]\n",
      " [ 2.66499996]\n",
      " [ 2.75      ]\n",
      " [ 2.66499996]\n",
      " [ 2.66499996]\n",
      " [ 2.4749999 ]\n",
      " [ 2.66000009]\n",
      " [ 2.66499996]\n",
      " [ 2.68000007]\n",
      " [ 2.68000007]\n",
      " [ 2.67000008]\n",
      " [ 2.68000007]\n",
      " [ 2.68000007]\n",
      " [ 2.68000007]\n",
      " [ 2.70000005]\n",
      " [ 2.70000005]\n",
      " [ 2.60500002]\n",
      " [ 2.74499989]\n",
      " [ 2.78999996]\n",
      " [ 2.875     ]\n",
      " [ 2.875     ]\n",
      " [ 2.82500005]\n",
      " [ 2.84500003]\n",
      " [ 2.83999991]\n",
      " [ 2.84500003]\n",
      " [ 2.75500011]\n",
      " [ 2.71499991]\n",
      " [ 2.70000005]\n",
      " [ 2.70000005]\n",
      " [ 2.5999999 ]\n",
      " [ 2.6500001 ]\n",
      " [ 2.67499995]\n",
      " [ 2.89499998]\n",
      " [ 2.5999999 ]\n",
      " [ 2.5999999 ]\n",
      " [ 2.61999989]]\n"
     ]
    }
   ],
   "source": [
    "weights = tf.constant(0.5, shape = (1, 1))\n",
    "graph = tf.matmul(input_ph, weights)\n",
    "result = session.run(graph, feed_dict = feed)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with variables & optimizers - Linear Regression\n",
    "Variables and optimizers serve as the heart of TenworFlow, they allow for a graph to 'learn'.\n",
    "\n",
    "Variables are the values in the graph that are updated with each epoc/iteration.\n",
    "* tf.Variables() - This is a wrapper on any constant in the graph\n",
    "\n",
    "Optimizers are the object that identifies how to update the variables to improve the values of a loss function.\n",
    "Many different optimizers, Adam is the most common:\n",
    "* tf.train.AdamOptimiser() - Usually you'll want to add a .minimize(loss_graph) to the end of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating the weights and biases for the model, weights randomly initialized, biases initialized to zero\n",
    "weights = tf.Variable(tf.random_normal(shape = (len(feature_cols), 1)))\n",
    "bias = tf.Variable(tf.zeros(shape = (len(label_cols),)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Don't need to add a new placeholder, since we already have one that will work in input_ph\n",
    "label_ph =  tf.placeholder(shape = (None, len(label_cols)), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating the linear regression model, and it's adjoining loss function\n",
    "lin_reg = tf.add(tf.matmul(input_ph, weights), bias)\n",
    "loss = tf.reduce_sum(tf.square(tf.subtract(label_ph, lin_reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating the optimizer, and setting its learning rate\n",
    "learn_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating the feed dictionary for the features & labels\n",
    "feed = {\n",
    "    input_ph: working_data[feature_cols].values,\n",
    "    label_ph: working_data[label_cols].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initializing variables prior to running model\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the graph, placeholders, feed dictionary, and whatnot ready, let's first see what the weights, bias, and current error values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.18011212]], dtype=float32), array([ 0.], dtype=float32), 868.24774]\n"
     ]
    }
   ],
   "source": [
    "print(session.run([weights, bias, loss], feed_dict = feed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do one iteration of learnning, and see how it changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.17911208]], dtype=float32), array([-0.001], dtype=float32), 857.77502]\n"
     ]
    }
   ],
   "source": [
    "session.run(optimizer, feed_dict = feed)\n",
    "print(session.run([weights, bias, loss], feed_dict = feed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do several iterations and see how the loss changes over each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x159214a8>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEQ97/z97LlpYHrcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1556be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_record = []\n",
    "iters = 100\n",
    "\n",
    "for i in range(iters):\n",
    "    _, iter_loss = session.run([optimizer, loss], feed_dict = feed)\n",
    "    loss_record.append(iter_loss)\n",
    "    \n",
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like healty model learning! Let's look at the current weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.09296572]], dtype=float32), array([-0.08696048], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(session.run([weights, bias])) ## Note, no feed dictionary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good are the predictions? Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    record_date  avg_price  avg_price_prior  avg_price_predict\n",
      "1    2015-04-02       5.75             5.71           6.153874\n",
      "2    2015-04-03       5.24             5.75           6.197593\n",
      "3    2015-04-04       5.74             5.24           5.640180\n",
      "4    2015-04-05       5.74             5.74           6.186663\n",
      "5    2015-04-06       5.64             5.74           6.186663\n",
      "6    2015-04-07       5.66             5.64           6.077366\n",
      "7    2015-04-08       5.64             5.66           6.099226\n",
      "8    2015-04-09       5.72             5.64           6.077366\n",
      "9    2015-04-10       5.48             5.72           6.164804\n",
      "10   2015-04-11       5.60             5.48           5.902492\n",
      "11   2015-04-12       5.51             5.60           6.033648\n",
      "12   2015-04-13       5.50             5.51           5.935281\n",
      "13   2015-04-14       5.49             5.50           5.924351\n",
      "14   2015-04-15       5.47             5.49           5.913421\n",
      "15   2015-04-16       5.46             5.47           5.891562\n",
      "16   2015-04-17       5.50             5.46           5.880632\n",
      "17   2015-04-18       5.47             5.50           5.924351\n",
      "18   2015-04-19       5.60             5.47           5.891562\n",
      "19   2015-04-20       5.09             5.60           6.033648\n",
      "20   2015-04-21       5.35             5.09           5.476235\n",
      "21   2015-04-22       5.00             5.35           5.760406\n",
      "22   2015-04-23       5.25             5.00           5.377868\n",
      "23   2015-04-24       5.30             5.25           5.651110\n",
      "24   2015-04-25       5.01             5.30           5.705758\n",
      "25   2015-04-26       5.20             5.01           5.388798\n",
      "26   2015-04-27       5.39             5.20           5.596461\n",
      "27   2015-04-28       5.05             5.39           5.804125\n",
      "28   2015-04-29       5.00             5.05           5.432517\n",
      "29   2015-04-30       5.19             5.00           5.377868\n",
      "30   2015-05-01       5.05             5.19           5.585532\n",
      "..          ...        ...              ...                ...\n",
      "885  2017-09-03       5.33             5.32           5.727618\n",
      "886  2017-09-04       5.36             5.33           5.738547\n",
      "887  2017-09-05       5.36             5.36           5.771336\n",
      "888  2017-09-06       5.34             5.36           5.771336\n",
      "889  2017-09-07       5.36             5.34           5.749477\n",
      "890  2017-09-08       5.36             5.36           5.771336\n",
      "891  2017-09-09       5.36             5.36           5.771336\n",
      "892  2017-09-10       5.40             5.36           5.771336\n",
      "893  2017-09-11       5.40             5.40           5.815055\n",
      "894  2017-09-12       5.21             5.40           5.815055\n",
      "895  2017-09-13       5.49             5.21           5.607391\n",
      "896  2017-09-14       5.58             5.49           5.913421\n",
      "897  2017-09-15       5.75             5.58           6.011788\n",
      "898  2017-09-16       5.75             5.75           6.197593\n",
      "899  2017-09-17       5.65             5.75           6.197593\n",
      "900  2017-09-18       5.69             5.65           6.088296\n",
      "901  2017-09-19       5.68             5.69           6.132015\n",
      "902  2017-09-20       5.69             5.68           6.121085\n",
      "903  2017-09-21       5.51             5.69           6.132015\n",
      "904  2017-09-22       5.43             5.51           5.935281\n",
      "905  2017-09-23       5.40             5.43           5.847843\n",
      "906  2017-09-24       5.40             5.40           5.815055\n",
      "907  2017-09-25       5.20             5.40           5.815055\n",
      "908  2017-09-26       5.30             5.20           5.596461\n",
      "909  2017-09-27       5.35             5.30           5.705758\n",
      "910  2017-09-28       5.79             5.35           5.760406\n",
      "911  2017-09-29       5.20             5.79           6.241311\n",
      "912  2017-09-30       5.20             5.20           5.596461\n",
      "913  2017-10-01       5.24             5.20           5.596461\n",
      "914  2017-10-02       5.30             5.24           5.640180\n",
      "\n",
      "[914 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "working_data.insert(len(working_data.columns), 'avg_price_predict', session.run(lin_reg, feed_dict = feed))\n",
    "print(working_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's pretty lousy, but maybe more learning will help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15c2bb38>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15b60208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iters = 10000\n",
    "\n",
    "for i in range(iters):\n",
    "    _, iter_loss = session.run([optimizer, loss], feed_dict = feed)\n",
    "    loss_record.append(iter_loss)\n",
    "    \n",
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    record_date  avg_price  avg_price_prior  avg_price_predict\n",
      "1    2015-04-02       5.75             5.71           5.648096\n",
      "2    2015-04-03       5.24             5.75           5.683928\n",
      "3    2015-04-04       5.74             5.24           5.227060\n",
      "4    2015-04-05       5.74             5.74           5.674970\n",
      "5    2015-04-06       5.64             5.74           5.674970\n",
      "6    2015-04-07       5.66             5.64           5.585388\n",
      "7    2015-04-08       5.64             5.66           5.603304\n",
      "8    2015-04-09       5.72             5.64           5.585388\n",
      "9    2015-04-10       5.48             5.72           5.657053\n",
      "10   2015-04-11       5.60             5.48           5.442057\n",
      "11   2015-04-12       5.51             5.60           5.549555\n",
      "12   2015-04-13       5.50             5.51           5.468932\n",
      "13   2015-04-14       5.49             5.50           5.459973\n",
      "14   2015-04-15       5.47             5.49           5.451015\n",
      "15   2015-04-16       5.46             5.47           5.433098\n",
      "16   2015-04-17       5.50             5.46           5.424140\n",
      "17   2015-04-18       5.47             5.50           5.459973\n",
      "18   2015-04-19       5.60             5.47           5.433098\n",
      "19   2015-04-20       5.09             5.60           5.549555\n",
      "20   2015-04-21       5.35             5.09           5.092687\n",
      "21   2015-04-22       5.00             5.35           5.325600\n",
      "22   2015-04-23       5.25             5.00           5.012063\n",
      "23   2015-04-24       5.30             5.25           5.236018\n",
      "24   2015-04-25       5.01             5.30           5.280809\n",
      "25   2015-04-26       5.20             5.01           5.021022\n",
      "26   2015-04-27       5.39             5.20           5.191227\n",
      "27   2015-04-28       5.05             5.39           5.361433\n",
      "28   2015-04-29       5.00             5.05           5.056854\n",
      "29   2015-04-30       5.19             5.00           5.012063\n",
      "30   2015-05-01       5.05             5.19           5.182269\n",
      "..          ...        ...              ...                ...\n",
      "885  2017-09-03       5.33             5.32           5.298726\n",
      "886  2017-09-04       5.36             5.33           5.307684\n",
      "887  2017-09-05       5.36             5.36           5.334558\n",
      "888  2017-09-06       5.34             5.36           5.334558\n",
      "889  2017-09-07       5.36             5.34           5.316642\n",
      "890  2017-09-08       5.36             5.36           5.334558\n",
      "891  2017-09-09       5.36             5.36           5.334558\n",
      "892  2017-09-10       5.40             5.36           5.334558\n",
      "893  2017-09-11       5.40             5.40           5.370391\n",
      "894  2017-09-12       5.21             5.40           5.370391\n",
      "895  2017-09-13       5.49             5.21           5.200185\n",
      "896  2017-09-14       5.58             5.49           5.451015\n",
      "897  2017-09-15       5.75             5.58           5.531639\n",
      "898  2017-09-16       5.75             5.75           5.683928\n",
      "899  2017-09-17       5.65             5.75           5.683928\n",
      "900  2017-09-18       5.69             5.65           5.594347\n",
      "901  2017-09-19       5.68             5.69           5.630179\n",
      "902  2017-09-20       5.69             5.68           5.621221\n",
      "903  2017-09-21       5.51             5.69           5.630179\n",
      "904  2017-09-22       5.43             5.51           5.468932\n",
      "905  2017-09-23       5.40             5.43           5.397265\n",
      "906  2017-09-24       5.40             5.40           5.370391\n",
      "907  2017-09-25       5.20             5.40           5.370391\n",
      "908  2017-09-26       5.30             5.20           5.191227\n",
      "909  2017-09-27       5.35             5.30           5.280809\n",
      "910  2017-09-28       5.79             5.35           5.325600\n",
      "911  2017-09-29       5.20             5.79           5.719761\n",
      "912  2017-09-30       5.20             5.20           5.191227\n",
      "913  2017-10-01       5.24             5.20           5.191227\n",
      "914  2017-10-02       5.30             5.24           5.227060\n",
      "\n",
      "[914 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "working_data['avg_price_predict'] = session.run(lin_reg, feed_dict = feed)\n",
    "print(working_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "The linear regression serves as the building block for neural networks. A few key points:\n",
    "\n",
    "1. tf.matmul() will be your main operation, remember that you are doing a matrix multiply across an entire layer, not node-by-node.\n",
    "2. The number of columns in the weights, and the number of biases will be equal to that layers number of nodes, usually greater than 1.\n",
    "3. After doing tf.matmul() [weights] and tf.add() [bias], you can then apply an activation function, depending on what kind of layer you want. tf.tanh() is a good one to start with.\n",
    "4. Make sure that the final layers activation matches the data youre working with! Sigmoid for dummy labels, linear (no activation) for continuous labels.\n",
    "\n",
    "Let's start by creating a simple feed-forward network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
